{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "\n",
    "def command_parser():\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('-f', dest='filename', help='Input file', required=True, type=str)\n",
    "\tparser.add_argument('--columns', help='Order of the columns in the file (eg: \"uirt\"), u for user, i for item, t for timestamp, r for rating. If r is not present a default rating of 1 is given to all interaction. If t is not present interactions are assumed to be in chronological order. Extra columns are ignored. Default: uit', default=\"uit\", type=str)\n",
    "\tparser.add_argument('--sep', help='Separator between the column. If unspecified pandas will try to guess the separator', default=\"\\s+\", type=str)\n",
    "\tparser.add_argument('--min_user_activity', help='Users with less interactions than this will be removed from the dataset. Default: 2', default=2, type=int)\n",
    "\tparser.add_argument('--min_item_pop', help='Items with less interactions than this will be removed from the dataset. Default: 5', default=5, type=int)\n",
    "\tparser.add_argument('--val_size', help='Number of users to put in the validation set. If in (0,1) it will be interpreted as the fraction of total number of users. Default: 0.1', default=0.1, type=float)\n",
    "\tparser.add_argument('--test_size', help='Number of users to put in the test set. If in (0,1) it will be interpreted as the fraction of total number of users. Default: 0.1', default=0.1, type=float)\n",
    "\tparser.add_argument('--seed', help='Seed for the random train/val/test split', default=1, type=int)\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\targs.dirname = os.path.dirname(os.path.abspath(args.filename)) + \"/\"\n",
    "\treturn args\n",
    "\n",
    "def warn_user(dirname):\n",
    "\t'''Ask user if he's sure to create files in that directory.\n",
    "\t'''\n",
    "\tprint('This program will create a lot of files and directories in ' + dirname)\n",
    "\tanswer = raw_input('Are you sure that you want to do that ? [y/n]')\n",
    "\tif answer != \"y\":\n",
    "\t\tsys.exit(0)\n",
    "\n",
    "def create_dirs(dirname):\n",
    "\tif not os.path.exists(dirname + \"data\"):\n",
    "\t\tos.makedirs(dirname + \"data\")\n",
    "\n",
    "\tif not os.path.exists(dirname + \"models\"):\n",
    "\t\tos.makedirs(dirname + \"models\")\n",
    "\n",
    "\tif not os.path.exists(dirname + \"results\"):\n",
    "\t\tos.makedirs(dirname + \"results\")\n",
    "\n",
    "def load_data(filename, columns, separator):\n",
    "\t''' Load the data from filename and sort it according to timestamp.\n",
    "\tReturns a dataframe with 3 columns: user_id, item_id, rating\n",
    "\t'''\n",
    "\n",
    "\tprint('Load data...')\n",
    "\tdata = pd.read_csv(filename, sep=separator, names=list(columns), index_col=False, usecols=range(len(columns)))\n",
    "\n",
    "\tif 'r' not in columns:\n",
    "\t\t# Add a column of default ratings\n",
    "\t\tdata['r'] = 1\n",
    "\n",
    "\t#if 't' in columns:\n",
    "\t\t# sort according to the timestamp column\n",
    "\t#\tif data['t'].dtype == np.int64: # probably a timestamp\n",
    "\t#\t\tdata['t'] = pd.to_datetime(data['t'], unit='s')\n",
    "\t#\telse:\n",
    "\t#\t\tdata['t'] = pd.to_datetime(data['t'])\n",
    "\t#\tprint('Sort data in chronological order...')\n",
    "\t#\tdata.sort_values('t', inplace=True)\n",
    "\n",
    "\treturn data\n",
    "\n",
    "def remove_rare_elements(data, min_user_activity, min_item_popularity):\n",
    "\t'''Removes user and items that appears in too few interactions.\n",
    "\tmin_user_activity is the minimum number of interaction that a user should have.\n",
    "\tmin_item_popularity is the minimum number of interaction that an item should have.\n",
    "\tNB: the constraint on item might not be strictly satisfied because rare users and items are removed in alternance,\n",
    "\tand the last removal of inactive users might create new rare items.\n",
    "\t'''\n",
    "\n",
    "\tprint('Remove inactive users and rare items...')\n",
    "\n",
    "\t#Remove inactive users a first time\n",
    "\tuser_activity = data.groupby('u').size()\n",
    "\tdata = data[np.in1d(data.u, user_activity[user_activity >= min_user_activity].index)]\n",
    "\t#Remove unpopular items\n",
    "\titem_popularity = data.groupby('i').size()\n",
    "\tdata = data[np.in1d(data.i, item_popularity[item_popularity >= min_item_popularity].index)]\n",
    "\t#Remove users that might have passed below the activity threshold due to the removal of rare items\n",
    "\tuser_activity = data.groupby('u').size()\n",
    "\tdata = data[np.in1d(data.u, user_activity[user_activity >= min_user_activity].index)]\n",
    "\n",
    "\treturn data\n",
    "\n",
    "def save_index_mapping(data, separator, dirname):\n",
    "\t''' Save the mapping of original user and item ids to numerical consecutive ids in dirname.\n",
    "\tNB: some users and items might have been removed in previous steps and will therefore not appear in the mapping.\n",
    "\t'''\n",
    "\n",
    "\tseparator = \"\\t\"\n",
    "\n",
    "\n",
    "\t# Pandas categorical type will create the numerical ids we want\n",
    "\tprint('Map original users and items ids to consecutive numerical ids...')\n",
    "\tdata['u_original'] = data['u'].astype('category')\n",
    "\tdata['i_original'] = data['i'].astype('category')\n",
    "\tdata['u'] = data['u_original'].cat.codes\n",
    "\tdata['i'] = data['i_original'].cat.codes\n",
    "\n",
    "\tprint('Save ids mapping to file...')\n",
    "\tuser_mapping = pd.DataFrame({'original_id' : data['u_original'], 'new_id': data['u']})\n",
    "\tuser_mapping.sort_values('original_id', inplace=True)\n",
    "\tuser_mapping.drop_duplicates(subset='original_id', inplace=True)\n",
    "\tuser_mapping.to_csv(dirname+\"data/user_id_mapping\", sep=separator, index=False)\n",
    "\n",
    "\titem_mapping = pd.DataFrame({'original_id' : data['i_original'], 'new_id': data['i']})\n",
    "\titem_mapping.sort_values('original_id', inplace=True)\n",
    "\titem_mapping.drop_duplicates(subset='original_id', inplace=True)\n",
    "\titem_mapping.to_csv(dirname+\"data/item_id_mapping\", sep=separator, index=False)\n",
    "\n",
    "\treturn data\n",
    "\n",
    "def split_data(data, nb_val_users, nb_test_users, dirname):\n",
    "\t'''Splits the data set into training, validation and test sets.\n",
    "\tEach user is in one and only one set.\n",
    "\tnb_val_users is the number of users to put in the validation set.\n",
    "\tnb_test_users is the number of users to put in the test set.\n",
    "\t'''\n",
    "\tnb_users = data['u'].nunique()\n",
    "\n",
    "\t# check if nb_val_user is specified as a fraction\n",
    "\tif nb_val_users < 1:\n",
    "\t\tnb_val_users = round(nb_val_users * nb_users)\n",
    "\tif nb_test_users < 1:\n",
    "\t\tnb_test_users = round(nb_test_users * nb_users)\n",
    "\tnb_test_users = int(nb_test_users)\n",
    "\tnb_val_users = int(nb_val_users)\n",
    "\n",
    "\tif nb_users <= nb_val_users+nb_test_users:\n",
    "\t\traise ValueError('Not enough users in the dataset: choose less users for validation and test splits')\n",
    "\n",
    "\tdef extract_n_users(df, n):\n",
    "\t\tusers_ids = np.random.choice(df['u'].unique(), n)\n",
    "\t\tn_set = df[df['u'].isin(users_ids)]\n",
    "\t\tremain_set = df.drop(n_set.index)\n",
    "\t\treturn n_set, remain_set\n",
    "\n",
    "\tprint('Split data into training, validation and test sets...')\n",
    "\ttest_set, tmp_set = extract_n_users(data, nb_test_users)\n",
    "\tval_set, train_set = extract_n_users(tmp_set, nb_val_users)\n",
    "\n",
    "\tprint('Save training, validation and test sets in the triplets format...')\n",
    "\ttrain_set.to_csv(dirname + \"data/train_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\tval_set.to_csv(dirname + \"data/val_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\ttest_set.to_csv(dirname + \"data/test_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\n",
    "\treturn train_set, val_set, test_set\n",
    "\n",
    "def split_data_timeline(data, nb_val_users, nb_test_users, dirname):\n",
    "\t'''Splits the data set into training, validation and test sets.\n",
    "\tEach user is in one and only one set.\n",
    "\tnb_val_users is the number of users to put in the validation set.\n",
    "\tnb_test_users is the number of users to put in the test set.\n",
    "\t'''\n",
    "\t#Create train and test set by 80-20\\% split\n",
    "\t# nb_users = data['u'].nunique()\n",
    "\t#\n",
    "\t# # check if nb_val_user is specified as a fraction\n",
    "\t# if nb_val_users < 1:\n",
    "\t# \tnb_val_users = round(nb_val_users * nb_users)\n",
    "\t# if nb_test_users < 1:\n",
    "\t# \tnb_test_users = round(nb_test_users * nb_users)\n",
    "\t# nb_test_users = int(nb_test_users)\n",
    "\t# nb_val_users = int(nb_val_users)\n",
    "\t#\n",
    "\t# if nb_users <= nb_val_users+nb_test_users:\n",
    "\t# \traise ValueError('Not enough users in the dataset: choose less users for validation and test splits')\n",
    "\t#\n",
    "\t# def extract_n_users(df, n):\n",
    "\t# \tusers_ids = np.random.choice(df['u'].unique(), n)\n",
    "\t# \tn_set = df[df['u'].isin(users_ids)]\n",
    "\t# \tremain_set = df.drop(n_set.index)\n",
    "\t# \treturn n_set, remain_set\n",
    "\t#\n",
    "\t# print('Split data into training, validation and test sets...')\n",
    "\t# test_set, tmp_set = extract_n_users(data, nb_test_users)\n",
    "\t# val_set, train_set = extract_n_users(tmp_set, nb_val_users)\n",
    "\n",
    "\tprint('Save training, validation and test sets in the triplets format...')\n",
    "\ttrain_set.to_csv(dirname + \"data/train_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\tval_set.to_csv(dirname + \"data/val_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\ttest_set.to_csv(dirname + \"data/test_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\n",
    "\treturn train_set, val_set, test_set\n",
    "\n",
    "def gen_sequences(data, half=False):\n",
    "\t'''Generates sequences of user actions from data.\n",
    "\teach sequence has the format [user_id, first_item_id, first_item_rating, 2nd_item_id, 2nd_item_rating, ...].\n",
    "\tIf half is True, cut the sequences to half their true length (useful to produce the extended training set).\n",
    "\t'''\n",
    "\tdata = data.sort_values('u', kind=\"mergesort\") # Mergesort is stable and keeps the time ordering\n",
    "\tseq = []\n",
    "\tprev_id = -1\n",
    "\tfor u, i, r in zip(data['u'], data['i'], data['r']):\n",
    "\t\tif u != prev_id:\n",
    "\t\t\tif len(seq) > 3:\n",
    "\t\t\t\tif half:\n",
    "\t\t\t\t\tseq = seq[:1+2*int((len(seq) - 1)/4)]\n",
    "\t\t\t\tyield seq\n",
    "\t\t\tprev_id = u\n",
    "\t\t\tseq = [u]\n",
    "\t\tseq.extend([i,r])\n",
    "\tif half:\n",
    "\t\tseq = seq[:1+2*int((len(seq) - 1)/4)]\n",
    "\tyield seq\n",
    "\n",
    "def make_sequence_format(train_set, val_set, test_set, dirname):\n",
    "\t'''Convert the train/validation/test sets in the sequence format and save them.\n",
    "\tAlso create the extended training sequences, which countains the first half of the sequences of users in the validation and test sets.\n",
    "\t'''\n",
    "\n",
    "\tprint('Save the training set in the sequences format...')\n",
    "\twith open(dirname+\"data/train_set_sequences\", \"w\") as f:\n",
    "\t\tfor s in gen_sequences(train_set):\n",
    "\t\t\tf.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "\tprint('Save the validation set in the sequences format...')\n",
    "\twith open(dirname+\"data/val_set_sequences\", \"w\") as f:\n",
    "\t\tfor s in gen_sequences(val_set):\n",
    "\t\t\tf.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "\tprint('Save the test set in the sequences format...')\n",
    "\twith open(dirname+\"data/test_set_sequences\", \"w\") as f:\n",
    "\t\tfor s in gen_sequences(test_set):\n",
    "\t\t\tf.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "\t# sequences+ contains all the sequences of train_set_sequences plus half the sequences of val and test sets\n",
    "\tprint('Save the extended training set in the sequences format...')\n",
    "\tcopyfile(dirname+\"data/train_set_sequences\", dirname+\"data/train_set_sequences+\")\n",
    "\twith open(dirname+\"data/train_set_sequences+\", \"a\") as f:\n",
    "\t\tfor s in gen_sequences(val_set, half=True):\n",
    "\t\t\tf.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\t\tfor s in gen_sequences(test_set, half=True):\n",
    "\t\t\tf.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "def save_data_stats(data, train_set, val_set, test_set, dirname):\n",
    "\tprint('Save stats...')\n",
    "\n",
    "\tdef _get_stats(df):\n",
    "\t\treturn \"\\t\".join(map(str, [df['u'].nunique(), df['i'].nunique(), len(df.index), df.groupby('u').size().max()]))\n",
    "\n",
    "\twith open(dirname+\"data/stats\", \"w\") as f:\n",
    "\t\tf.write(\"set\\tn_users\\tn_items\\tn_interactions\\tlongest_sequence\\n\")\n",
    "\t\tf.write(\"Full\\t\"+ _get_stats(data) + \"\\n\")\n",
    "\t\tf.write(\"Train\\t\"+ _get_stats(train_set) + \"\\n\")\n",
    "\t\tf.write(\"Val\\t\"+ _get_stats(val_set) + \"\\n\")\n",
    "\t\tf.write(\"Test\\t\"+ _get_stats(test_set) + \"\\n\")\n",
    "\n",
    "def make_readme(dirname, val_set, test_set):\n",
    "\tdata_readme = '''The following files were automatically generated by preprocess.py\n",
    "\n",
    "\tuser_id_mapping\n",
    "\t\tmapping between the users ids in the original dataset and the new users ids.\n",
    "\t\tthe first column contains the new id and the second the original id.\n",
    "\t\tInactive users might have been deleted from the original, and they will therefore not appear in the id mapping.\n",
    "\n",
    "\titem_id_mapping\n",
    "\t\tIdem for item ids.\n",
    "\n",
    "\ttrain_set_triplets\n",
    "\t\tTraining set in the triplets format.\n",
    "\t\tEach line is a user item interaction in the form (user_id, item_id, rating).\n",
    "\t\tInteractions are listed in chronological order.\n",
    "\n",
    "\ttrain_set_sequences\n",
    "\t\tTraining set in the sequence format.\n",
    "\t\tEach line contains all the interactions of a user in the form (user_id, first_item_id, first_rating, 2nd_item_id, 2nd_rating, ...).\n",
    "\n",
    "\ttrain_set_sequences+\n",
    "\t\tExtended training set in the sequence format.\n",
    "\t\tThe extended training set contains all the training set plus the first half of the interactions of each users in the validation and testing set.\n",
    "\n",
    "\tval_set_triplets\n",
    "\t\tValidation set in the triplets format\n",
    "\n",
    "\tval_set_triplets\n",
    "\t\tValidation set in the sequence format\n",
    "\n",
    "\ttest_set_triplets\n",
    "\t\tTest set in the triplets format\n",
    "\n",
    "\ttest_set_triplets\n",
    "\t\tTest set in the sequence format\n",
    "\n",
    "\tstats\n",
    "\t\tContains some informations about the dataset.\n",
    "\n",
    "\tThe training, validation and test sets are obtain by randomly partitioning the users and all their interactions into 3 sets.\n",
    "\tThe validation set contains {n_val} users, the test_set {n_test} users and the train set all the other users.\n",
    "\n",
    "\t'''.format(n_val=str(val_set['u'].nunique()), n_test=str(test_set['u'].nunique()))\n",
    "\n",
    "\tresults_readme = '''The format of the results file is the following\n",
    "\tEach line correspond to one model, with the fields being:\n",
    "\t\tNumber of epochs\n",
    "\t\tprecision\n",
    "\t\tsps\n",
    "\t\tuser coverage\n",
    "\t\tnumber of unique items in the test set\n",
    "\t\tnumber of unique items in the recommendations\n",
    "\t\tnumber of unique items in the succesful recommendations\n",
    "\t\tnumber of unique items in the short-term test set (when the goal is to predict precisely the next item)\n",
    "\t\tnumber of unique items in the successful short-term recommendations\n",
    "\t\trecall\n",
    "\t\tNDCG\n",
    "\tNB: all the metrics are computed \"@10\"\n",
    "\t'''\n",
    "\n",
    "\twith open(dirname+\"data/README\", \"w\") as f:\n",
    "\t\tf.write(data_readme)\n",
    "\twith open(dirname+\"results/README\", \"w\") as f:\n",
    "\t\tf.write(results_readme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "args = command_parser()\n",
    "args.filename = '/Users/kanika/Box Sync/UserRecommendation/PACE2017/data/traindata_small.txt'\n",
    "args.columns = \"ui\"\n",
    "args.sep = \",\"\n",
    "data = load_data(args.filename, args.columns, args.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     u    i  r\n",
      "88   1   88  1\n",
      "89   1    1  1\n",
      "90   1    2  1\n",
      "91   1   89  1\n",
      "92   1   90  1\n",
      "93   1   91  1\n",
      "94   1   92  1\n",
      "95   1    9  1\n",
      "96   1   93  1\n",
      "97   1   11  1\n",
      "98   1   94  1\n",
      "99   1   95  1\n",
      "100  1   21  1\n",
      "101  1   27  1\n",
      "102  1   96  1\n",
      "103  1   97  1\n",
      "104  1   98  1\n",
      "105  1   32  1\n",
      "106  1   34  1\n",
      "107  1   35  1\n",
      "108  1   99  1\n",
      "109  1   37  1\n",
      "110  1  100  1\n",
      "111  1  101  1\n",
      "112  1  102  1\n",
      "113  1   41  1\n",
      "114  1  103  1\n",
      "115  1   43  1\n",
      "116  1  104  1\n",
      "117  1   46  1\n",
      "..  ..  ... ..\n",
      "134  1  109  1\n",
      "135  1  110  1\n",
      "136  1  111  1\n",
      "137  1   66  1\n",
      "138  1   67  1\n",
      "139  1  112  1\n",
      "140  1  113  1\n",
      "141  1   68  1\n",
      "142  1  114  1\n",
      "143  1  115  1\n",
      "144  1  116  1\n",
      "145  1   70  1\n",
      "146  1  117  1\n",
      "147  1   72  1\n",
      "148  1   73  1\n",
      "149  1  118  1\n",
      "150  1  119  1\n",
      "151  1   77  1\n",
      "152  1   78  1\n",
      "153  1   79  1\n",
      "154  1  120  1\n",
      "155  1  121  1\n",
      "156  1   82  1\n",
      "157  1  122  1\n",
      "158  1   85  1\n",
      "159  1  123  1\n",
      "160  1   86  1\n",
      "161  1  124  1\n",
      "162  1  125  1\n",
      "163  1   87  1\n",
      "\n",
      "[76 rows x 3 columns]\n",
      "     u    i  r\n",
      "135  1  110  1\n",
      "136  1  111  1\n",
      "139  1  112  1\n",
      "140  1  113  1\n",
      "142  1  114  1\n",
      "143  1  115  1\n",
      "144  1  116  1\n",
      "146  1  117  1\n",
      "149  1  118  1\n",
      "150  1  119  1\n",
      "154  1  120  1\n",
      "155  1  121  1\n",
      "157  1  122  1\n",
      "159  1  123  1\n",
      "161  1  124  1\n",
      "162  1  125  1\n"
     ]
    }
   ],
   "source": [
    "counts = data['u'].map(data.groupby('u').apply(len))\n",
    "ranks = data.groupby('u')['i'].rank(method='first')\n",
    "#ranks = data.index[data.groupby('u')['i']]\n",
    "#print(counts)\n",
    "#print(ranks)\n",
    "#print((ranks / counts) > 0.8)\n",
    "\n",
    "\n",
    "print(data.loc[data['u'] == 0])\n",
    "#print(data.loc[data['u'] == 1]['i'].index)\n",
    "test_data = data[(ranks / counts) > 0.8]\n",
    "print(test_data.loc[test_data['u'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "from sys import executable\n",
    "print(executable)\n",
    "import pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
